{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from azureml.core import Workspace, Experiment\n",
        "\n",
        "ws = Workspace.get(name=\"capstone-ml\")\n",
        "exp = Experiment(workspace=ws, name=\"uda-cap\")\n",
        "\n",
        "print('Workspace name: ' + ws.name, \n",
        "      'Azure region: ' + ws.location, \n",
        "      'Subscription id: ' + ws.subscription_id, \n",
        "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
        "\n",
        "run = exp.start_logging()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Workspace name: capstone-ml\n",
            "Azure region: eastus\n",
            "Subscription id: b09642af-8e07-4efe-80f5-7f7e59fb2cf2\n",
            "Resource group: uda-cap\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1604077626610
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\n",
        "experiment_name = 'capstone-ml'\n",
        "\n",
        "experiment=Experiment(ws, experiment_name)"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1604077627655
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "\n",
        "\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "cluster_name = \"cap-cluster\"\n",
        "\n",
        "try:\n",
        "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
        "    print('I found the existing cluster, So I am using it.')\n",
        "except ComputeTargetException:\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
        "                                                           vm_priority = 'lowpriority',\n",
        "                                                           max_nodes=4)\n",
        "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
        "    compute_target.wait_for_completion(show_output=True, min_node_count = 1, timeout_in_minutes = 10)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I found the existing cluster, So I am using it.\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1604077628339
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.widgets import RunDetails\n",
        "from azureml.train.sklearn import SKLearn\n",
        "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
        "from azureml.train.hyperdrive.policy import BanditPolicy\n",
        "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
        "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
        "from azureml.train.hyperdrive.parameter_expressions import uniform, choice\n",
        "import os\n",
        "import pandas as pd\n",
        "from train import clean_data\n",
        "from azureml.data.dataset_factory import TabularDatasetFactory\n",
        "from sklearn.model_selection import train_test_split\n",
        "import logging\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.widgets import RunDetails\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/pandas/core/generic.py:6786: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._update_inplace(new_data)\n",
            "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/pandas/core/generic.py:5208: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n",
            "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/pandas/core/frame.py:4117: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1604077635253
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://cap.blob.core.windows.net/cap/train.csv'\n",
        "\n",
        "ds = TabularDatasetFactory.from_delimited_files(url)\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1604077635674
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clean the dataset with the clean_data function, imported from the training script"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = clean_data(ds)\n",
        "y = y.drop(y.index[0])\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1604077635813
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "   Married  Dependents  Education  Self_Employed  ApplicantIncome  \\\n2        1           0          1              1             3000   \n3        1           0          0              0             2583   \n4        0           0          1              0             6000   \n5        1           2          1              1             5417   \n6        1           0          0              0             2333   \n\n   CoapplicantIncome  LoanAmount  Loan_Amount_Term  Credit_History  \\\n2                0.0        66.0             360.0             1.0   \n3             2358.0       120.0             360.0             1.0   \n4                0.0       141.0             360.0             1.0   \n5             4196.0       267.0             360.0             1.0   \n6             1516.0        95.0             360.0             1.0   \n\n   Property_Area  \n2              2  \n3              2  \n4              2  \n5              2  \n6              2  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Married</th>\n      <th>Dependents</th>\n      <th>Education</th>\n      <th>Self_Employed</th>\n      <th>ApplicantIncome</th>\n      <th>CoapplicantIncome</th>\n      <th>LoanAmount</th>\n      <th>Loan_Amount_Term</th>\n      <th>Credit_History</th>\n      <th>Property_Area</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3000</td>\n      <td>0.0</td>\n      <td>66.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2583</td>\n      <td>2358.0</td>\n      <td>120.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>6000</td>\n      <td>0.0</td>\n      <td>141.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5417</td>\n      <td>4196.0</td>\n      <td>267.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2333</td>\n      <td>1516.0</td>\n      <td>95.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1604077636580
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.concat([x_train, y_train], axis=1)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1604077636878
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "     Married  Dependents  Education  Self_Employed  ApplicantIncome  \\\n329        0           0          1              0             2500   \n54         1           1          1              1            11500   \n230        1           1          1              0             2491   \n274        1           2          1              0             3900   \n210        0           0          1              0            10000   \n\n     CoapplicantIncome  LoanAmount  Loan_Amount_Term  Credit_History  \\\n329                0.0        67.0             360.0             1.0   \n54                 0.0       286.0             360.0             0.0   \n230             2054.0       104.0             360.0             1.0   \n274                0.0        90.0             360.0             1.0   \n210                0.0       214.0             360.0             1.0   \n\n     Property_Area  Loan_Status  \n329              2          1.0  \n54               2          0.0  \n230              1          1.0  \n274              1          1.0  \n210              1          0.0  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Married</th>\n      <th>Dependents</th>\n      <th>Education</th>\n      <th>Self_Employed</th>\n      <th>ApplicantIncome</th>\n      <th>CoapplicantIncome</th>\n      <th>LoanAmount</th>\n      <th>Loan_Amount_Term</th>\n      <th>Credit_History</th>\n      <th>Property_Area</th>\n      <th>Loan_Status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>329</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2500</td>\n      <td>0.0</td>\n      <td>67.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>11500</td>\n      <td>0.0</td>\n      <td>286.0</td>\n      <td>360.0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>230</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2491</td>\n      <td>2054.0</td>\n      <td>104.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>274</th>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3900</td>\n      <td>0.0</td>\n      <td>90.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>210</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>10000</td>\n      <td>0.0</td>\n      <td>214.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1604077637041
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "   Loan_Status\n2          1.0\n3          1.0\n4          1.0\n5          1.0\n6          1.0",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Loan_Status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1604077637307
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AutoML Configuration"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AutoML Setting"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "automl_settings = {\n",
        "    \"featurization\": \"auto\",\n",
        "    \"n_cross_validations\": 4,\n",
        "    \"experiment_timeout_minutes\": 30,\n",
        "    \"enable_early_stopping\": True,\n",
        "    \"verbosity\": logging.INFO,\n",
        "}"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1604077266236
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AutoML Configuration"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# automl_config = AutoMLConfig(\n",
        "#     task=\"classification\",\n",
        "#     training_data=df_train,\n",
        "#     label_column_name=y,\n",
        "#     primary_metric=\"AUC_weighted\",\n",
        "#     **automl_settings\n",
        "# )"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1604077637397
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "automl_config = AutoMLConfig(\n",
        "    experiment_timeout_minutes=30,\n",
        "    task=\"classification\",\n",
        "    primary_metric=\"accuracy\",\n",
        "    training_data=df_train,\n",
        "    label_column_name=\"Loan_Status\",\n",
        "    n_cross_validations=5)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1604077637717
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submit Experiment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "automl_run = exp.submit(automl_config, show_output=True)\n",
        "# RunDetails(hyperdrive_run).show()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on local machine\n",
            "Parent Run ID: AutoML_461e7c11-28bd-464d-8215-67062724e3b1\n",
            "\n",
            "Current status: DatasetEvaluation. Gathering dataset statistics.\n",
            "Current status: FeaturesGeneration. Generating features for the dataset.\n",
            "Current status: DatasetFeaturization. Beginning to fit featurizers and featurize the dataset.\n",
            "Current status: DatasetFeaturizationCompleted. Completed fit featurizers and featurizing the dataset.\n",
            "Current status: DatasetCrossValidationSplit. Generating individually featurized CV splits.\n",
            "\n",
            "****************************************************************************************************\n",
            "DATA GUARDRAILS: \n",
            "\n",
            "TYPE:         Class balancing detection\n",
            "STATUS:       PASSED\n",
            "DESCRIPTION:  Your inputs were analyzed, and all classes are balanced in your training data.\n",
            "              Learn more about imbalanced data: https://aka.ms/AutomatedMLImbalancedData\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "TYPE:         Missing feature values imputation\n",
            "STATUS:       PASSED\n",
            "DESCRIPTION:  No feature missing values were detected in the training data.\n",
            "              Learn more about missing value imputation: https://aka.ms/AutomatedMLFeaturization\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "TYPE:         High cardinality feature detection\n",
            "STATUS:       PASSED\n",
            "DESCRIPTION:  Your inputs were analyzed, and no high cardinality features were detected.\n",
            "              Learn more about high cardinality feature handling: https://aka.ms/AutomatedMLFeaturization\n",
            "\n",
            "****************************************************************************************************\n",
            "Current status: ModelSelection. Beginning model selection.\n",
            "\n",
            "****************************************************************************************************\n",
            "ITERATION: The iteration being evaluated.\n",
            "PIPELINE: A summary description of the pipeline being evaluated.\n",
            "DURATION: Time taken for the current iteration.\n",
            "METRIC: The result of computing score on the fitted pipeline.\n",
            "BEST: The best observed score thus far.\n",
            "****************************************************************************************************\n",
            "\n",
            " ITERATION   PIPELINE                                       DURATION      METRIC      BEST\n",
            "         0   MaxAbsScaler LightGBM                          0:00:23       0.7733    0.7733\n",
            "         1   MaxAbsScaler XGBoostClassifier                 0:00:20       0.7872    0.7872\n",
            "         2   MaxAbsScaler RandomForest                      0:00:34       0.7788    0.7872\n",
            "         3   MaxAbsScaler RandomForest                      0:00:33       0.7676    0.7872\n",
            "         4   MaxAbsScaler SGD                               0:00:20       0.7705    0.7872\n",
            "         5   MaxAbsScaler SGD                               0:00:20       0.8012    0.8012\n",
            "         6   MaxAbsScaler ExtremeRandomTrees                0:00:20       0.7984    0.8012\n",
            "         7   MaxAbsScaler ExtremeRandomTrees                0:00:20       0.7928    0.8012\n",
            "         8   MaxAbsScaler ExtremeRandomTrees                0:00:20       0.8012    0.8012\n",
            "         9   MaxAbsScaler ExtremeRandomTrees                0:00:20       0.7370    0.8012\n",
            "        10   MaxAbsScaler SGD                               0:00:20       0.7984    0.8012\n",
            "        11   MaxAbsScaler SGD                               0:00:19       0.7928    0.8012\n",
            "        12   MaxAbsScaler RandomForest                      0:00:20       0.7649    0.8012\n",
            "        13   StandardScalerWrapper ExtremeRandomTrees       0:00:20       0.7928    0.8012\n",
            "        14   MaxAbsScaler RandomForest                      0:00:19       0.7287    0.8012\n",
            "        15   MaxAbsScaler SGD                               0:00:19       0.7760    0.8012\n",
            "        16   MaxAbsScaler RandomForest                      0:00:20       0.7592    0.8012\n",
            "        17   MaxAbsScaler ExtremeRandomTrees                0:00:19       0.8040    0.8040\n",
            "        18   SparseNormalizer ExtremeRandomTrees            0:00:20       0.7394    0.8040\n",
            "        19   MaxAbsScaler SGD                               0:00:20       0.8012    0.8040\n",
            "        20   MaxAbsScaler ExtremeRandomTrees                0:00:20       0.7342    0.8040\n",
            "        21   MaxAbsScaler RandomForest                      0:00:20       0.6836    0.8040\n",
            "        22   MaxAbsScaler LightGBM                          0:00:20       0.7984    0.8040\n",
            "        23   MaxAbsScaler RandomForest                      0:00:20       0.6613    0.8040\n",
            "        24   MaxAbsScaler ExtremeRandomTrees                0:00:19       0.8012    0.8040\n",
            "        25   MaxAbsScaler ExtremeRandomTrees                0:00:20       0.6613    0.8040\n",
            "        26   SparseNormalizer LightGBM                      0:00:20       0.7705    0.8040\n",
            "        27   SparseNormalizer XGBoostClassifier             0:00:20       0.7873    0.8040\n",
            "        28   StandardScalerWrapper RandomForest             0:00:20       0.7760    0.8040\n",
            "        29   StandardScalerWrapper ExtremeRandomTrees       0:00:23       0.6892    0.8040\n",
            "        30   StandardScalerWrapper ExtremeRandomTrees       0:00:21       0.8012    0.8040\n",
            "        31   StandardScalerWrapper XGBoostClassifier        0:00:34       0.7984    0.8040\n",
            "        32   MaxAbsScaler LinearSVM                         0:00:23       0.8012    0.8040\n",
            "        33   MaxAbsScaler LightGBM                          0:00:20       0.7928    0.8040\n",
            "        34   MaxAbsScaler LightGBM                          0:00:20       0.7732    0.8040\n",
            "        35   SparseNormalizer ExtremeRandomTrees            0:00:30       0.6613    0.8040\n",
            "        36   MaxAbsScaler LogisticRegression                0:00:20       0.7928    0.8040\n",
            "        37   StandardScalerWrapper ExtremeRandomTrees       0:00:22       0.7928    0.8040\n",
            "        38   SparseNormalizer XGBoostClassifier             0:00:20       0.7733    0.8040\n",
            "        39   StandardScalerWrapper ExtremeRandomTrees       0:00:19       0.7984    0.8040\n",
            "        40   StandardScalerWrapper ExtremeRandomTrees       0:00:23       0.7760    0.8040\n",
            "        41   StandardScalerWrapper LogisticRegression       0:00:20       0.7928    0.8040\n",
            "        42   MaxAbsScaler RandomForest                      0:00:19       0.7928    0.8040\n",
            "        43   MaxAbsScaler ExtremeRandomTrees                0:00:20       0.7956    0.8040\n",
            "        44   SparseNormalizer XGBoostClassifier             0:00:20       0.7733    0.8040\n",
            "        45   StandardScalerWrapper ExtremeRandomTrees       0:00:20       0.8012    0.8040\n",
            "        46   SparseNormalizer XGBoostClassifier             0:00:20       0.7732    0.8040\n",
            "        47   StandardScalerWrapper XGBoostClassifier        0:00:20       0.8012    0.8040\n",
            "        48   StandardScalerWrapper LightGBM                 0:00:20       0.7900    0.8040\n",
            "        49   StandardScalerWrapper XGBoostClassifier        0:00:20       0.7901    0.8040\n",
            "        50   MaxAbsScaler ExtremeRandomTrees                0:00:21       0.7788    0.8040\n",
            "        51   StandardScalerWrapper GradientBoosting         0:00:20       0.8012    0.8040\n",
            "        52   MaxAbsScaler GradientBoosting                  0:00:20       0.7705    0.8040\n",
            "        53   StandardScalerWrapper LightGBM                 0:00:18       0.7928    0.8040\n",
            "        54   MaxAbsScaler LogisticRegression                0:00:19       0.7984    0.8040\n",
            "        55   StandardScalerWrapper ExtremeRandomTrees       0:00:20       0.8012    0.8040\n",
            "        56   SparseNormalizer XGBoostClassifier             0:00:19       0.7957    0.8040\n",
            "        57   StandardScalerWrapper XGBoostClassifier        0:00:34       0.7872    0.8040\n",
            "        58   MaxAbsScaler LightGBM                          0:00:32       0.7789    0.8040\n",
            "        59   MaxAbsScaler SGD                               0:00:19       0.7734    0.8040\n",
            "        60   SparseNormalizer ExtremeRandomTrees            0:00:21       0.8012    0.8040\n",
            "        61   StandardScalerWrapper XGBoostClassifier        0:00:20       0.8012    0.8040\n",
            "        62   StandardScalerWrapper SGD                      0:00:20       0.7566    0.8040\n",
            "        63   MaxAbsScaler RandomForest                      0:00:22       0.7872    0.8040\n",
            "        64   StandardScalerWrapper XGBoostClassifier        0:00:20       0.8012    0.8040\n",
            "        65   StandardScalerWrapper ExtremeRandomTrees       0:00:22       0.7676    0.8040\n",
            "        66   MaxAbsScaler RandomForest                      0:00:23       0.7984    0.8040\n",
            "        67   StandardScalerWrapper LightGBM                 0:00:20       0.7704    0.8040\n",
            "        68   StandardScalerWrapper LightGBM                 0:00:19       0.7901    0.8040\n",
            "        69   StandardScalerWrapper XGBoostClassifier        0:00:20       0.7984    0.8040\n",
            "        70   StandardScalerWrapper ExtremeRandomTrees       0:00:21       0.7984    0.8040\n",
            "        71   StandardScalerWrapper XGBoostClassifier        0:00:20       0.7929    0.8040\n",
            "        72   StandardScalerWrapper RandomForest             0:00:19       0.7620    0.8040\n",
            "        73   MaxAbsScaler LightGBM                          0:00:18       0.7591    0.8040\n",
            "        74   MaxAbsScaler LogisticRegression                0:00:20       0.7928    0.8040\n",
            "        75   MaxAbsScaler ExtremeRandomTrees                0:00:20       0.8012    0.8040\n",
            "        76   VotingEnsemble                                 0:00:41       0.8012    0.8040\n",
            "        77   StackEnsemble                                  0:00:43       0.8012    0.8040\n",
            "Stopping criteria reached at iteration 78. Ending experiment.\n",
            "****************************************************************************************************\n",
            "Current status: BestRunExplainModel. Best run model explanations started\n",
            "Current status: ModelExplanationDataSetSetup. Model explanations data setup completed\n",
            "Current status: PickSurrogateModel. Choosing LightGBM as the surrogate model for explanations\n",
            "Current status: EngineeredFeatureExplanations. Computation of engineered features started\n",
            "Current status: EngineeredFeatureExplanations. Computation of engineered features completed\n",
            "Current status: BestRunExplainModel. Best run model explanations completed\n",
            "****************************************************************************************************\n"
          ]
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1604079722500
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}